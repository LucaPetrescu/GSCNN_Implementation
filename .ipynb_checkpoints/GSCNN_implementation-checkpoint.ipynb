{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecec23b0",
   "metadata": {},
   "source": [
    "## Implementation of GSCNN in a Jupyter Notebook\n",
    "\n",
    "In this Jupyter Notebook, we will implement GSCNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5caf7312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0121768",
   "metadata": {},
   "source": [
    "The device we would like to train our network on would ideally to be out graphics card. However, in case we do not \n",
    "have a graphics card, then the device will be the CPU by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f08a6caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b36d245",
   "metadata": {},
   "source": [
    "#### Part 1 - Implementing the Network Architecture\n",
    "\n",
    "Here we define the basic architecture of the network. In includes several classes, but we will explain each one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a48d4296",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crop(nn.Module):\n",
    "    def __init__(self, axis, offset):\n",
    "        super(Crop, self).__init__()\n",
    "        self.axis = axis\n",
    "        self.offset = offset\n",
    "\n",
    "    def forward(self, x, ref):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x: input layer\n",
    "        :param ref: reference usually data in\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for axis in range(self.axis, x.dim()):\n",
    "            ref_size = ref.size(axis)\n",
    "            indices = torch.arange(self.offset, self.offset + ref_size).long()\n",
    "            indices = x.data.new().resize_(indices.size()).copy_(indices).long()\n",
    "            x = x.index_select(axis, Variable(indices))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe72a79e",
   "metadata": {},
   "source": [
    "The **Crop** class is used to extract a subregion from an input tensor based on a reference tensor. \n",
    "It is a useful tool for data augmentation and other tasks that require extracting specific elements from a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68b670cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIdentity(nn.Module):\n",
    "    def __init__(self, axis, offset):\n",
    "        super(MyIdentity, self).__init__()\n",
    "        self.axis = axis\n",
    "        self.offset = offset\n",
    "\n",
    "    def forward(self, x, ref):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x: input layer\n",
    "        :param ref: reference usually data in\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1256e6c4",
   "metadata": {},
   "source": [
    "The **MyIdentity** class in the provided code snippet is a custom neural network layer that simply returns the input tensor x without any modification. It takes two arguments in its constructor: **axis** and **offset**. These arguments are not used in the forward method and are likely just there to maintain consistency with the **Crop** class.\n",
    "\n",
    "The **forward** method of the **MyIdentity** class simply returns the input tensor **x** without any further processing. This indicates that the layer does not perform any transformation on the input data. It is essentially a pass-through layer that allows data to flow through the network without any changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9f94888",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SideOutputCrop(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the original implementation ConvTranspose2d (fixed) and crops\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_output, kernel_sz=None, stride=None, upconv_pad=0, do_crops=True):\n",
    "        super(SideOutputCrop, self).__init__()\n",
    "        self._do_crops = do_crops\n",
    "        self.conv = nn.Conv2d(num_output, out_channels=1, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\n",
    "        if kernel_sz is not None:\n",
    "            self.upsample = True\n",
    "            self.upsampled = nn.ConvTranspose2d(1, out_channels=1, kernel_size=kernel_sz, stride=stride,\n",
    "                                                padding=upconv_pad,\n",
    "                                                bias=False)\n",
    "            ##doing crops\n",
    "            if self._do_crops:\n",
    "                self.crops = Crop(2, offset=kernel_sz // 4)\n",
    "            else:\n",
    "                self.crops = MyIdentity(None, None)\n",
    "        else:\n",
    "            self.upsample = False\n",
    "\n",
    "    def forward(self, res, reference=None):\n",
    "        side_output = self.conv(res)\n",
    "        if self.upsample:\n",
    "            side_output = self.upsampled(side_output)\n",
    "            side_output = self.crops(side_output, reference)\n",
    "\n",
    "        return side_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b123072",
   "metadata": {},
   "source": [
    "The **SideOutputCrop** class is used to generate side output for a CNN. It can optionally apply upsampling and cropping to the side output. Upsampling is useful for increasing the spatial resolution of the side output, while cropping can be used to remove irrelevant or redundant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82b4e61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _AtrousSpatialPyramidPoolingModule(nn.Module):\n",
    "    '''\n",
    "    operations performed:\n",
    "      1x1 x depth\n",
    "      3x3 x depth dilation 6\n",
    "      3x3 x depth dilation 12\n",
    "      3x3 x depth dilation 18\n",
    "      image pooling\n",
    "      concatenate all together\n",
    "      Final 1x1 conv\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_dim, reduction_dim=256, output_stride=16, rates=[6, 12, 18]):\n",
    "        super(_AtrousSpatialPyramidPoolingModule, self).__init__()\n",
    "\n",
    "        # Check if we are using distributed BN and use the nn from encoding.nn\n",
    "        # library rather than using standard pytorch.nn\n",
    "\n",
    "        if output_stride == 8:\n",
    "            rates = [2 * r for r in rates]\n",
    "        elif output_stride == 16:\n",
    "            pass\n",
    "        else:\n",
    "            raise 'output stride of {} not supported'.format(output_stride)\n",
    "\n",
    "        self.features = []\n",
    "        # 1x1\n",
    "        self.features.append(\n",
    "            nn.Sequential(nn.Conv2d(in_dim, reduction_dim, kernel_size=1, bias=False),\n",
    "                          Norm2d(reduction_dim), nn.ReLU(inplace=True)))\n",
    "        # other rates\n",
    "        for r in rates:\n",
    "            self.features.append(nn.Sequential(\n",
    "                nn.Conv2d(in_dim, reduction_dim, kernel_size=3,\n",
    "                          dilation=r, padding=r, bias=False),\n",
    "                Norm2d(reduction_dim),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ))\n",
    "        self.features = torch.nn.ModuleList(self.features)\n",
    "\n",
    "        # img level features\n",
    "        self.img_pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self.img_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, reduction_dim, kernel_size=1, bias=False),\n",
    "            Norm2d(reduction_dim), nn.ReLU(inplace=True))\n",
    "        self.edge_conv = nn.Sequential(\n",
    "            nn.Conv2d(1, reduction_dim, kernel_size=1, bias=False),\n",
    "            Norm2d(reduction_dim), nn.ReLU(inplace=True))\n",
    "         \n",
    "\n",
    "    def forward(self, x, edge):\n",
    "        x_size = x.size()\n",
    "\n",
    "        img_features = self.img_pooling(x)\n",
    "        img_features = self.img_conv(img_features)\n",
    "        img_features = F.interpolate(img_features, x_size[2:],\n",
    "                                     mode='bilinear',align_corners=True)\n",
    "        out = img_features\n",
    "\n",
    "        edge_features = F.interpolate(edge, x_size[2:],\n",
    "                                      mode='bilinear',align_corners=True)\n",
    "        edge_features = self.edge_conv(edge_features)\n",
    "        out = torch.cat((out, edge_features), 1)\n",
    "\n",
    "        for f in self.features:\n",
    "            y = f(x)\n",
    "            out = torch.cat((out, y), 1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935eb20d",
   "metadata": {},
   "source": [
    "The **_AtrousSpatialPyramidPoolingModule** class in the provided code snippet is a neural network module that performs atrous spatial pyramid pooling (ASPP) on the input feature map. ASPP is a technique for aggregating information from multiple feature scales in a single module. This can be useful for improving the performance of semantic segmentation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e050349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from network.wider_resnet import WiderResNetA2\n",
    "\n",
    "class GSCNN(nn.Module):\n",
    "    '''\n",
    "    Wide_resnet version of DeepLabV3\n",
    "    mod1\n",
    "    pool2\n",
    "    mod2 str2\n",
    "    pool3\n",
    "    mod3-7\n",
    "\n",
    "      structure: [3, 3, 6, 3, 1, 1]\n",
    "      channels = [(128, 128), (256, 256), (512, 512), (512, 1024), (512, 1024, 2048),\n",
    "                  (1024, 2048, 4096)]\n",
    "    '''\n",
    "\n",
    "    def __init__(self, num_classes, trunk=None, criterion=None):\n",
    "        \n",
    "        super(GSCNN, self).__init__()\n",
    "        self.criterion = criterion\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        wide_resnet = WiderResNetA2(classes=1000, dilation=True)\n",
    "        wide_resnet = torch.nn.DataParallel(wide_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f1f874c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "WiderResNetA2.__init__() missing 1 required positional argument: 'structure'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m network \u001b[38;5;241m=\u001b[39m GSCNN(\u001b[38;5;241m4\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 23\u001b[0m, in \u001b[0;36mGSCNN.__init__\u001b[1;34m(self, num_classes, trunk, criterion)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion \u001b[38;5;241m=\u001b[39m criterion\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m num_classes\n\u001b[1;32m---> 23\u001b[0m wide_resnet \u001b[38;5;241m=\u001b[39m WiderResNetA2(classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, dilation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     24\u001b[0m wide_resnet \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mDataParallel(wide_resnet)\n",
      "\u001b[1;31mTypeError\u001b[0m: WiderResNetA2.__init__() missing 1 required positional argument: 'structure'"
     ]
    }
   ],
   "source": [
    "network = GSCNN(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33f7d60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
